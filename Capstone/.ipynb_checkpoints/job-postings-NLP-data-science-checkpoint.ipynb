{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Language of Jobpostings\n",
    "### Using datascience to understand the datascience industry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/capstoneppt-title.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/netflix-because-youwatched.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine a world without recommendations, having to search for new movies only using titles:\n",
    "\n",
    "- \"Terminator\" with time travel\n",
    "- \"Star Wars\" in medieval times\n",
    "- \"Sleepless in Seattle\" without Seattle, and without being sleepless\n",
    "\n",
    "Frustrating, inefficient, and slow\n",
    "Even if the genre, actor, or tone is known, how can we find it purely by the title? \n",
    "\n",
    "### This is exactly the current process for Job Searching\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science is broad\n",
    "\n",
    "Data science, first, is a term with two ambiguous terms. Second, as of 2016 it's a very popular topic and technique uesd in a variety of industries. As seen in the screenshot below, it can be a scientist, an engineer, or maybe even a product or relationship manager. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/Sample_data_science_search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Above: a sample job search for \"data science\" in late october 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Searching for new jobs can be an arduous process. The diligent can get through 10-20 per week, and the hit rate may not be that high. If the job search goes for 2 months, thats around 150 job postings. From my initial search, there are 1000s of data science/ data analyst positions, and many more similar positions under business and product engineers. So can data science help data scientists find jobs?\n",
    "\n",
    "Assumption: NLP can be used with the detailed job descriptions to \n",
    "1. predict job titles (if any) for job descriptions\n",
    "2. in process of predicting job titles, which titles are similiar and why?\n",
    "3. word patterns for various jobs\n",
    "4. what can we learn about the machine learning industry from these job postings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/capstoneppt-scraping.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Terms Searched for:**\n",
    "- \"Data Science\"\n",
    "- \"Data Scientist\"\n",
    "- \"Data Analyst\"\n",
    "- \"Data Engineer\"\n",
    "- \"Business Analyst\"\n",
    "- \"Machine Learning\"\n",
    "- \"Statistics\"\n",
    "- \"Product Analyst\"\n",
    "- \"Deep Learning\"\n",
    "\n",
    "**Cities/Locations**\n",
    "- San Francisco, CA\n",
    "- Mountain View, CA\n",
    "- Seattle, WA\n",
    "- Los Angeles, CA\n",
    "- Boston, MA\n",
    "- New York, NY\n",
    "- Philadelphia, PN\n",
    "- Washington, DC\n",
    "- Atlanta, GA\n",
    "- Houston, TX\n",
    "- Austin, TX\n",
    "- Chicago, IL\n",
    "- Minneapolis, MN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Picking Common Titles : \"Data Analyst of Operations\" vs. \"Health Data Analyst III\"?\n",
    "\n",
    "Every company is essentially its own country with its own titles. A bank's entry level position might be \"Assistance Vice President\" and a consulting firm might be \"business analyst\". Many new tech companies with flat organizations are notorious for having titles that do not matter; a data analyst could be a PhD with 12 years of work experience. And for every industry, there's another layer of specific lingo. \n",
    "\n",
    "#### Sample Job Posting Titles:\n",
    "```\n",
    "['Clinical Practice Specialist - 4 Hope' 'WEB DEVELOPER'\n",
    " 'Analyst, Marketing Analytics' 'Data Engineer'\n",
    " 'Application Implementation Specialist' 'Consulting Analyst'\n",
    " 'Data Integration Engineer' 'Health Care Junior Business Analyst'\n",
    " 'Solutions Architect' 'Senior Application Developer' 'Data Analyst Junior'\n",
    " 'Clinical Informatics Data Analyst' 'Junior Data Scientist, Marketing'\n",
    " 'Business Intelligence Analyst' 'SEO Specialist'\n",
    " 'Market and Cost Intelligence Analyst' 'Data Analyst'\n",
    " 'Application Developer/Analyst' 'Senior Data Analyst, Discovery Analytics'\n",
    " 'Big Data Senior Consultant - Information Delivery'\n",
    " 'Business Intelligence Engineer - Amazon Restaurants'\n",
    " 'Principle Software Engineer'\n",
    " 'Healthcare & Finance Analyst, Performance Measurement'\n",
    " 'Application Development Programmer Specialist'\n",
    " 'Senior Analyst, Strategy & Operations']\n",
    "```\n",
    "#### \"Base\" Titles\n",
    "All the titles were extracted to isolate the base title, such as \n",
    "    \"Engineer\"\n",
    "    \"Analyst\"\n",
    "    \"Architect\"\n",
    "    \"Developer\"\n",
    "    \"Scientist\"\n",
    "\n",
    "#### \"Expanded\" Titles\n",
    "All the titles were extracted to have one layer of detail above the base title, such as \"Data Engineer\", \"Software Engineer\", \"Learning Engineer\", \"Data Analyst\", \"Business Analyst\", \"Senior Analyst\"\n",
    "\n",
    "Only job postings that have over 100 records will isolated from the rest of the master dataset. This reduces the original 22k count to 12k. \n",
    "\n",
    "    business analyst, \n",
    "    data analyst\n",
    "    software engineer\n",
    "    data scientist\n",
    "    data engineer\n",
    "    analyst\n",
    "    systems analyst\n",
    "    development engineer\n",
    "    senior consultant\n",
    "    marketing specialist\n",
    "    director\n",
    "    product manager\n",
    "    manager\n",
    "    senior analyst\n",
    "    intelligence analyst\n",
    "    research scientist\n",
    "    marketing analyst\n",
    "    data architect\n",
    "    operations analyst\n",
    "    project manager\n",
    "    solution architect\n",
    "    product analyst\n",
    "    financial analyst\n",
    "    senior associate\n",
    "    learning engineer\n",
    "    research analyst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which titles have the most jobs?\n",
    "\n",
    "Because our search terms were specifically for data analyst, and business analyst, and since they are the most \"entry-level\" of all the titles, they have the most postings in our data gathering approach. And because of all the data science related search terminology, data scientist, data engineer, software engineer were all top hits. Interestingly, marketing specialist has a number of postings as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/vega0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which postings are longer (by char length) ?:\n",
    "\n",
    "Not surprisingly, more senior positions tended to have more text than entry level positions. One other interesting observation is that the positions in the hotter markets tended to have shorter descriptions, such as software engineer, research scientist, data engineer, and learning engineer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/vega1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where are these postings from ?\n",
    "\n",
    "Size of the circle is the number of postings in the grid of titles and state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/vega2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rough Metric : Machine Learning Terms occurance in postings\n",
    "\n",
    "In a job posting, what is the density of job terms? The following plot shows the breakout. \n",
    "\n",
    "Terms:\n",
    "- machine learning\n",
    "- data science\n",
    "- regression\n",
    "- bayes\n",
    "- sklearn\n",
    "- data scientist\n",
    "- neural networks\n",
    "- ' R '\n",
    "\n",
    "Big bubbles close to the Y axis means that there a number of postings that had only a few of the data-science related terms. Distributions farther away from the Y-axis means that there's a nice distribution of data science postings at a number of job postings.\n",
    "\n",
    "#### Data scientist, data engineer, software engineer, research scientist, data architect\n",
    "has a good distribution of data science related terms ( has even-sized dots all the way out to the right)\n",
    "\n",
    "#### Data analyst, business analyst, data engineer\n",
    "Has a big cluster of jobs with few terms (big dot near the axis), but also has some jobs with a number of data science terms (lots of dots to the right). The problem with these positions, is that it's a mixed bag. There's positions that are very data sciency, but there's a lot of noise to get through to find them "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/vega4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which companies had the most posts within our data-science search terms?\n",
    "\n",
    "From the search terms cited above, which companies showed up the most frequent in the results? Or in other words, who has the widest and largest tech opportunities?\n",
    "\n",
    "Deloitte, Amazon, Google, Uber, United Healthcare, and a number of staffing agencies were top on the list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/vega6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which Companies have the most Machine-Learning Terminology?\n",
    "\n",
    "Similarly, which of the companies have the highest density of Machine Learning terminology? Amazon and Deloitte currently lead with the most postings and highest density. United Healthcare also has a number of postings with high data-science relevancy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/vega7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How much of the job posting text should be put through processing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/three-texts.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will use the above class to parse the full job descriptions into:\n",
    "\n",
    "1. Full job posting\n",
    "2. Just the new-line identified line items\n",
    "3. Any phrases following after specific prepositions to identify key skills\n",
    "\n",
    "These parsed text files are saved in list data types to be tokenized\n",
    "\n",
    "#### Total Job Posting --> Line Item Only --> Hard Skill only\n",
    "\n",
    "Make 3 tiers of text, the original posting, newline single item collections, and hard skills only text fields for NLP analysis. Job descriptions often have excessively long introductions or context descriptions about the company or specifically have to call out equal opportunity disclaimers, or they talk about the growing industry. Often job postings have a listing broken by new lines, items such as the following:\n",
    "\n",
    "- Experienced in SQL, Java\n",
    "- Delivering products on tight deadlines\n",
    "- Undertanding of big data tools such as Hadoop or Hive\n",
    "- Communication skills are a must\n",
    "\n",
    "These are often the meat of the resume, and have been pulled out of the text with some clever list manipulation and text tools. At the next level down, \"hard skills\" or specific tools are isolated with the following leading phrases: \n",
    "**to**, **in**, **experience with**, **knowledge of**,**including**, **requirements**. Splitting on these terms will isolate the following:\n",
    "\n",
    "- Experienced **in SQL, Java**\n",
    "- Delivering products on tight deadlines\n",
    "- **Undertanding of big data tools such as Hadoop or Hive**\n",
    "- Communication skills are a must\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. NLP - Word Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![alt text](img/wordvec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following NLP Parser class takes in a list of text and automates the following tasks:\n",
    "\n",
    "- Vectorizing of the different texts into arrays/matrices of words using Count Vectorizer\n",
    "- Apply stop words filter to get rid of common words\n",
    "- Corpus creation - an optimized word count data structure (combination of dictionary and tuples)\n",
    "- vocab creation - remaining unique words\n",
    "\n",
    "These word features will become the features that will be used for predicting expanded title names. Only the top **20,000** word features (by occurance count) will be used for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NLP_parser(text):\n",
    "    # using count vectorizer to turn the cleaned job descriptions into feadutes\n",
    "    cvec = CountVectorizer(stop_words='english', lowercase=True,ngram_range=(1,1))\n",
    "    start_time = datetime.datetime.now()\n",
    "    cvec.fit(text)\n",
    "\n",
    "    \n",
    "    #create a data from from the elements generated from Count Vectorizer\n",
    "    cdf  = pd.DataFrame(cvec.transform(text).todense(),\n",
    "                 columns=cvec.get_feature_names())\n",
    "\n",
    "    #keeping the top 20,000 features (or words)\n",
    "    summary = cdf.sum().sort_values(ascending = False)\n",
    "    keep_cols = summary[:20000].index\n",
    "    cdf_to_merge = cdf[keep_cols]\n",
    "\n",
    "\n",
    "    # since some of the words are reserved, adding NLP_ as a prefix to stop any type of compiler issues\n",
    "    cdf_to_merge.columns = ['nlp_'+x for x in cdf_to_merge.columns]\n",
    "    return cdf_to_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_df = NLP_parser(fulltext)\n",
    "LI_df = NLP_parser(LItext)\n",
    "HS_df = NLP_parser(HStext)\n",
    "\n",
    "print full_df.shape, LI_df.shape, HS_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot: Top word counts per different position titles\n",
    "\n",
    "Below is the frequency of words for different job titles. Most frequent words are closer the X-axis at the bottom. Top words across all job postings are: experience, data, work, team, skills, management, ability. And these are only the top words. The model we will train will look at a vocabulary of 20,000 to look at the occurane rate.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/vega14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/scoring-explanation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Classification Function - setup to take models for one vs. rest library\n",
    "\n",
    "#### Multiclass classification\n",
    "\n",
    "The classification approach will be for 26 different classes. The modeling will be performed by using logistic regression class by class, or basically 26 different models. This is for one modeling run. So for however many runs that need to be tested it will be 26 times. \n",
    "\n",
    "Python sklearn has a prebuilt function called :\n",
    "        - OneVsRestClassifier\n",
    "\n",
    "This OneVsRestClassifier is a generic utility that can take a model and run it multiple times for multiclass classification. For each training, the model will run 26 different tests such as : data analyst vs. rest, then engineer vs. rest, and then data scientist vs. the rest. To reduce the code bloat, the following function was designed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Baseline Modeling results: Logistic Regression without Regularization\n",
    "\n",
    "### Logistic Classification training data set score: 0.97\n",
    "\n",
    "### Logistic Classification test data set score: 0.641\n",
    "\n",
    "### Verdict: train score not equal to test score = Overfit model\n",
    "\n",
    "The overall score is above the baseline, the large difference in the training score vs. the actual test score seems to indicate overfitting. Overfitting means that the initial model is too specific to the training data. Once \"new\" data is put through the model (aka our \"test\" set), the score drops, because its too unlike the training data. A robust general model will have similiar scores no matter the data that is supplied to it. \n",
    "\n",
    "To make our model more general and more robust, regularization will be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Regularization ?\n",
    "\n",
    "Since we are doing text analysis, every word is a feature. As we add more and more \"rows\" or job postings with text, the vocabulary continues to increase, so our feature count will inevitably increase. As a result: \n",
    "\n",
    "#### For text analysis, we will generally have more features than records (unless limited). Lots of these features will be obscure.\n",
    "\n",
    "There will be a number of features that will throw the data off, or will not be useful. Text and language is very complex, so its difficult to know specifically why. For example a lot of postings will have disclaimers about race, or ethnicity that need to be ignored. Maybe all these postings will have phrases like \"only serious applicants taken seriously\". Regularization, specifically the Lasso technique will be used to penalize some of these extra features that aren't needed.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Regularization_(mathematics)#Regularization_in_statistics_and_machine_learning\n",
    "\n",
    "#### Low / No regularization: same as regular Logistic Regression\n",
    "\n",
    "#### High regularization: penalize all features, will try and drop unnecesary features within a rough threshold\n",
    "\n",
    "The problem with high regularization, is that if we go too high, we will drop ALL our features and there will be nothing left to use for predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Model Overview : Optimizing with Regularization\n",
    "\n",
    "Dataset | Log | L1 C=0.01 | L1 C=0.1 | L1 C=1.0 | L1 C=10.0 | L1 C=100.0 | Random Forest|\n",
    "--------|-----|-----------|----------|----------|-----------|------------|-----------|\n",
    "Full Text|x|x|x|x|x|x|x\n",
    "Line Items Only|x|x|x|x|x|x|x\n",
    "Hard Skills only |x|x|x|x|x|x|x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization constant C = 1 / lambda\n",
    "\n",
    "In most literature, lambda is the mathematic regularization constant with no units. This term is used to control the regularization effect talked about above. So more regularization = bigger lambda = lower C values.\n",
    "\n",
    "#### Modeling code:\n",
    "\n",
    "Will run 21 times for the above combinations to find the optimal regularization constant. The below is a sample code block that will be run for multiple times. The logistic regression will use One Vs Rest to do multi-class classification, and will allow us to use the liblinear solver.\n",
    "\n",
    "26 different classifications\n",
    "21 different models\n",
    "2  different runs for train and test\n",
    "5  cross val\n",
    "\n",
    "### = 5460 different models to be run\n",
    "\n",
    "As a result, AWS was used to run all the different combinations. A 32GB ram 16-thread machine ran all these models in about 2 hours. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores of different Statistical Models : (with different levels of regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Regularized Model Scores\n",
    "\n",
    "ref field is the code for the name of the model test being run. \n",
    "\n",
    "- LI = line item, HS = Hard skill, FULL = full posting text\n",
    "- RFC = random forest classifier\n",
    "- L1100 = Lasso regression C = 100\n",
    "- L11.0 = Lasso regression C = 1.0\n",
    "etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/log_res_table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of Regularized Scores : Effect of regularization constant C\n",
    "\n",
    "The plot was converted to a log based plot to easily see the difference. When trying to plot 0.1, 1.0, 10, 100, 1000, the details get condensed into a small part of the graph. The blip in the middle is due to trying to plot C = 0. \n",
    "\n",
    "#### LogC = 2 / C = 100\n",
    "At this point the regularization constant has no effect at increasing values. The scores don't change. This is almost equivalent to having regularization at all\n",
    "\n",
    "#### LogC = -4 / C = 0.01\n",
    "At this point the scores converge, but so much detail is lost, that the model starts performing very poorly. Essential and important detail is being lost\n",
    "\n",
    "#### LogC = -2 / C = 0.1\n",
    "At this point the scores converge, the scores are similar, but we don't lose a lot of detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/vega12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Regularization constant logC = -2, or C = 0.1 (where training and test scores converge)\n",
    "\n",
    "#### @ C = 0.1 Train score ~0.60\n",
    "#### @ C = 0.1 test score ~0.53\n",
    "#### Full text performs better than the Line-items or Hard-Skill text as source material\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Modeling Results - Probabilities and Predictions \n",
    "\n",
    "After processing we get the following formats of results. Predictions and probabilities. Each row of the matrix = a record. \n",
    "\n",
    "Predictions:\n",
    "\n",
    "    y = [analyst | engineer  | scientist | developer]\n",
    "        [ 0.,        1.,        0.,      0.]\n",
    "        [ 1.,        0.,        0.,      0.]\n",
    "        \n",
    "Predicted Probaility:\n",
    "\n",
    "    y = [analyst | engineer | scientist | developer]\n",
    "        [ .21,      .87,      0.01,     0.31]\n",
    "        [ .93,       .34,      0.14,     0.45]\n",
    "\n",
    "Using the probability matrix, we can glean a number of insights, which will be listed below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title similarity - based on probabilities for actual vs. predicted\n",
    "\n",
    "Since we have similar options for different postings, we can make a correlation matrix based on the probabilities\n",
    "\n",
    "#### Sample posting with different probabilities:\n",
    "```\n",
    "Post #1 : analyst ('analyst', 0.97124201971635582)\n",
    "\t('data analyst', 0.09224897854402879)\n",
    "\t('marketing analyst', 0.025353417134555477)\n",
    "\t('intelligence analyst', 0.00020392029014586449)\n",
    "Post #2 : data engineer ('data engineer', 0.94262491122939063)\n",
    "\t('software engineer', 0.032009124051313669)\n",
    "\t('data scientist', 0.0073729054476535522)\n",
    "\t('development engineer', 0.0065031250470106144)\n",
    "```\n",
    "\n",
    "## Actual to Predicted Title Probabilities (what was often mistaken?)\n",
    "#### Excluded matches (e.g. pred. data analyst == actual data analyst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/vega13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some key metrics that are used to understand classification problems are the following:\n",
    "\n",
    "- **Accuracy:** Overall, how often is the classifier correct?\n",
    "- **Misclassification Rate:** Overall, how often is it wrong?\n",
    "- **Recall:** When it's actually yes, how often does it predict yes?\n",
    "- **Precision:** When it predicts yes, how often is it correct?\n",
    "\n",
    "### For each job title, a separate confusion matrix was run and collected below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/vega19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "**'Analysts' have the lowest recall rates overall.** Which means our model predicts these titles, it has only a 70% chance of predicting the right answer. And this is inline with our other analysis above.\n",
    "\n",
    "**'Business analyst' and 'Data analyst' have the highest error rate.** Since these titles are often confused with other analysts, data engineers, and data scientists, this also is inline with out other observations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roc Curve - overall scoring per 26 predicted titles\n",
    "\n",
    "The following curve plots the ROC for all 26 classes. It's a lot to look at, but the key take away (similiar to the confusion matrix metrics, is that some of the titles are well predicted, and other ones do much worse. The legend was excluded due to the number of curves being plotted.\n",
    "\n",
    "The lower curves (closer to the diagonal) are the worst performing titles, which are:\n",
    "\n",
    "#### Poor performing titles\n",
    "- business analyst\n",
    "- data analyst\n",
    "- data scientist\n",
    "- data engineer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/UntitledROC_all.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What titles were wrongly predicted, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Poor performing titles\n",
    "- business analyst\n",
    "- data analyst\n",
    "- data scientist\n",
    "- data engineer\n",
    "\n",
    "Since our average score was ~0.60 for prediction. This chart breaks out the remaining 40% of the test dataset that was mis-predicted. Looking at business analyst, the most common wrong predictions were data analyst, analyst, and systems analyst. With the analyst positions being entry level, this is understandable. Software engineer was most wrongly predicted as data engineer, again, with our targeted job search focus, there's a lot of similiarity here.\n",
    "\n",
    "Looking at actual data analysts, the most common wrong predictions were business analyst, data scientist, data engineer, and intelligence analyst. This could be that these data analyst positions were basically data scientist postings, but perhaps the requirements were not as stringent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/vega11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Words for Predictions - some samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the statistical model, the top features (in this case, words) can be extracted to know what words are the top indicators that influence the outcome. Larger magnitude of scores means a stronger influence.\n",
    "\n",
    "- Blue: Logistic Regression for classification **without** any type of regularization\n",
    "\n",
    "- Orange: Logistic Regression for classification **with** any type of regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Predicting Words for Data Scientist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/vega15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the top predicting words for data scientist are phd, python, statistics, predictive, which coincides with its more focused nature, these requirements are more definitive for the Data Scientist role "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Predicting Words for Data Engineer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/vega16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the top predicting words for data engineer are etl, warehouse, pipelines, hadoop, shell, bhase, integrating. This dovetails nicely with the infrastructure responsibilities related to Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Predicting Words for Research Scientist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/vega18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the top words for Research scientist are: research, matlab, scientific, amazon, networks, leader, mathematical, conferences, and programming. For our job postings, it seems also that amazon and yahoo had a number of these positions compared to other companies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Conclusions and Takeaways\n",
    "\n",
    "### Modeling \n",
    "\n",
    "\n",
    "- **Data science is still an ambiguous field** the two main avenues that this is being worked into is from the software/data engineering end and from the analytics business oriented end. One end is seeking to build the next level of intelligence and automation, and the other end is trying to understand the sea of data that is being generated daily. As a results, these responsibilities will creep into existing roles until new fully defined rolls can be established. This can be seen in the 60% predictive rate. These dedicated and specific positions are forming, but there is still a lot of ambiguity and cross over that exists as companies figure this out.\n",
    "\n",
    "\n",
    "- **There is no bad score** - What I love about this problem is that there is no right answer. If the predicition score was incredibly low, than job titles mean nothing, and all job descriptions are not correlated. If The prediction score was 100% that would mean that we only have to apply to data scientists / data analysts and take solace that no other jobs slip through our fingers. At ~60%, that would mean that as you apply by title to various jobs, there's about 40% of other opportunities out there that are miss named, or that you couldn't find because it was named incorrectly.  \n",
    "\n",
    "\n",
    "- **Prediction could be better, but uncertain how much** With a rough score of 0.5-0.6 on the prediction, its a good sign that more detail could be mined out from the job postings. Ensemble methods were avoided because the goal was to understand the inner workings, not to achieve the highest prediction score. \n",
    "\n",
    "\n",
    "- **Additional Models** - Further work into decision tree classifiers may yield more insights. But I believe further feature engineering with the actual text would yield the most benefit.\n",
    "\n",
    "\n",
    "### NLP for Job Postings\n",
    "\n",
    "- **Jobs are abstract items, inherently difficult ** - Jobs in general are difficult to define at times. If there isn't a specific purpose or skill that is required such as \"blacksmith\" or \"must know piping\", many jobs sound the same at the high level, talking to people, setting up meetings ,and getting a wide variety taskes complete. It's not always easy to summarize a job, much less translate your current experience into a different position. Being very organized with monthly team meetings, and designing websites sometimes gets abstracted to : people-person, go-getter, not afraid to be technical.\n",
    "\n",
    "\n",
    "- **Garbage in, Garbage Out** - and unfortunately for job postings, there's a lot of unnecessary words. There's phrases that are added for legal or generic requirementslike \"be a strong communicator\", or \"be a hard worker\", but is a lot of noise compared to the rest of the data  \n",
    "\n",
    "\n",
    "- **Removing disclaimers** - every company always has a \"fluff\" piece near the beginning of the posting and a disclaimer near the bottom, disclaiming anything about race, gender, and equal opportunity, its difficult to isolate these common words to isolate the target\n",
    "\n",
    "\n",
    "- **Responsiblities / Requirements / Must Haves / What you'll do/ What you'd need** This section is often the key area to scrutinize to determine the minimum requirements for applying. Unfortunately every hip-recruiter or company has decided to rename this section to something else. Sometimes jobs will have \"must haves\" and \"nice to have\" or sometimes its built into each line.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps:\n",
    "\n",
    "- **Problem is now setup for Topic Modeling** - since it is shown that there is about a 40% ambiguity rating in the overall data science market from looking at pure words, it's the perfect time to move to advanced NLP or topic modeling. The topic modeling will extract more of the latent detail and provide a psuedo-metric to measure on. Instead of \"how many times does data & science appear\", the question will be \"how close to Business-reporting topic does this job posting land?\" \"how close is it to statistical-data-prediction topic?\" I've prototyped this process, but this in itself is a whole other capstone. \n",
    "\n",
    "\n",
    "- This was drafted in this analysis but will be covered elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Overview of the topic modeling that was tested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/lda-diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Topics that were intepretted from the word groups out of LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/topic-sample.png)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [dsi]",
   "language": "python",
   "name": "Python [dsi]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
