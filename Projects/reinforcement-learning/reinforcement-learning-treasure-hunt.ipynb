{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "\n",
    "Code adapted from a tutorial:\n",
    "https://github.com/ml4a/ml4a-guides/blob/master/notebooks/q_learning.ipynb\n",
    "\n",
    "- Made a few changes\n",
    "- Added my own narrative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Introduction: Q Learning in a Nutshell\n",
    "\n",
    "https://en.wikipedia.org/wiki/Q-learning\n",
    "\n",
    "#### Wikipedia defines the Q-learning Algorithm as follows:\n",
    "    The problem model consists of an agent, states S and a set of actions per state A. By performing an action A, the agent can move from state to state. Executing an action in a specific state provides the agent with a reward (a numerical score). The goal of the agent is to maximize its total reward. It does this by learning which action is optimal for each state. The action that is optimal for each state is the action that has the highest long-term reward.\n",
    "    \n",
    "My version in the context of the game of checkers: a players have \"turns\". At any given turn in the game. A player can make a \"action\" and these can have immediate benefits (take an enemy piece), but can have long-term implications (enemy becomes queen next turn). The algorithm will teach the computer player the best choice to make at any given point in the game. Here's some examples:\n",
    "\n",
    "Some sample states / actions\n",
    "\n",
    "Game |  Player | possible state | possible action\n",
    "--|--|--|-----\n",
    "Chess |Player that has pieces| has 1 knight | move forward L\n",
    "Chess |Player that has pieces| has 1 knight | move backward L\n",
    "Chess |Player that has pieces| has rook or bishop | move rook forward x spaces\n",
    "Chess |Player that has pieces| has rook or bishop | move rook back x spaces\n",
    "Chess |Player that has pieces| has rook or bishop | move bishop diagonaly back x spaces\n",
    "\n",
    "etc..\n",
    "\n",
    "The **Q value** is the relative benefit or score for each of these actions\n",
    "\n",
    "Game |  Player | possible state | possible action | Q score\n",
    "--|--|--|---|--\n",
    "Chess |Player that has pieces| has 1 knight | move forward L | 12\n",
    "Chess |Player that has pieces| has 1 knight | move backward L | -5\n",
    "Chess |Player that has pieces| has rook or bishop | move rook forward x spaces | 20\n",
    "Chess |Player that has pieces| has rook or bishop | move rook back x spaces | -10\n",
    "Chess |Player that has pieces| has rook or bishop | move bishop diagonaly back x spaces | 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    def __init__(self, grid):\n",
    "        self.grid = grid\n",
    "        self.n_rows = len(grid)\n",
    "        self.n_cols = len(grid[0])\n",
    "        self.positions = self._positions()\n",
    "        self.starting_positions = [p for p in self.positions\n",
    "                                   if not self.is_terminal_state(p)]\n",
    "\n",
    "    def actions(self, pos):\n",
    "        \"\"\"possible actions for a state (position)\"\"\"\n",
    "        r, c = pos\n",
    "        actions = []\n",
    "        if r > 0:\n",
    "            actions.append('up')\n",
    "        if r < self.n_rows - 1:\n",
    "            actions.append('down')\n",
    "        if c > 0:\n",
    "            actions.append('left')\n",
    "        if c < self.n_cols - 1:\n",
    "            actions.append('right')\n",
    "        return actions\n",
    "\n",
    "    def value(self, pos):\n",
    "        \"\"\"retrieve the reward value for a position\"\"\"\n",
    "        r, c = pos\n",
    "        return self.grid[r][c]\n",
    "\n",
    "    def _positions(self):\n",
    "        \"\"\"all positions\"\"\"\n",
    "        positions = []\n",
    "        for r, row in enumerate(self.grid):\n",
    "            for c, _ in enumerate(row):\n",
    "                positions.append((r,c))\n",
    "        return positions\n",
    "\n",
    "    def is_terminal_state(self, state):\n",
    "        \"\"\"tell us if the state ends the game\"\"\"\n",
    "        val = self.value(state)\n",
    "        return val is None or val > 0\n",
    "\n",
    "    def reward(self, state):\n",
    "        \"\"\"the reward of a state:\n",
    "        -1 if it's a hole,\n",
    "        -1 if it's an empty space (to penalize each move),\n",
    "        otherwise, the value of the state\"\"\"\n",
    "        val = self.value(state)\n",
    "        if val is None or val == 0:\n",
    "            return -1\n",
    "        return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QLearner():\n",
    "    def __init__(self, state, environment, rewards, discount=0.5, explore=0.5, learning_rate=1):\n",
    "        \"\"\"\n",
    "        - state: the agent's starting state\n",
    "        - rewards: a reward function, taking a state as input, or a mapping of states to a reward value\n",
    "        - discount: how much the agent values future rewards over immediate rewards\n",
    "        - explore: with what probability the agent \"explores\", i.e. chooses a random action\n",
    "        - learning_rate: how quickly the agent learns. For deterministic environments (like ours), this should be left at 1\n",
    "        \"\"\"\n",
    "        self.discount = discount\n",
    "        self.explore = explore\n",
    "        self.learning_rate = learning_rate\n",
    "        self.R = rewards.get if isinstance(rewards, dict) else rewards\n",
    "\n",
    "        # our state is just our position\n",
    "        self.state = state\n",
    "        self.reward = 0\n",
    "        self.env = environment\n",
    "\n",
    "        # initialize Q\n",
    "        self.Q = {}\n",
    "\n",
    "    def reset(self, state):\n",
    "        self.state = state\n",
    "        self.reward = 0\n",
    "\n",
    "    def actions(self, state):\n",
    "        return self.env.actions(state)\n",
    "\n",
    "    def _take_action(self, state, action):\n",
    "        r, c = state\n",
    "        if action == 'up':\n",
    "            r -= 1\n",
    "        elif action == 'down':\n",
    "            r += 1\n",
    "        elif action == 'right':\n",
    "            c += 1\n",
    "        elif action == 'left':\n",
    "            c -= 1\n",
    "\n",
    "        # return new state\n",
    "        return (r,c)\n",
    "\n",
    "    def step(self, action=None):\n",
    "        \"\"\"take an action\"\"\"\n",
    "        # check possible actions given state\n",
    "        actions = self.actions(self.state)\n",
    "\n",
    "        # if this is the first time in this state,\n",
    "        # initialize possible actions\n",
    "        if self.state not in self.Q:\n",
    "            self.Q[self.state] = {a: 0 for a in actions}\n",
    "\n",
    "        if action is None:\n",
    "            if random.random() < self.explore:\n",
    "                action = random.choice(actions)\n",
    "            else:\n",
    "                action = self._best_action(self.state)\n",
    "        elif action not in actions:\n",
    "            raise ValueError('unrecognized action!')\n",
    "\n",
    "        # remember this state and action\n",
    "        # so we can later remember\n",
    "        # \"from this state, taking this action is this valuable\"\n",
    "        prev_state = self.state\n",
    "\n",
    "        # update state\n",
    "        self.state = self._take_action(self.state, action)\n",
    "\n",
    "        # update the previous state/action based on what we've learned\n",
    "        self._learn(prev_state, action, self.state)\n",
    "        return action\n",
    "\n",
    "    def _best_action(self, state):\n",
    "        \"\"\"choose the best action given a state\"\"\"\n",
    "        actions_rewards = list(self.Q[state].items())\n",
    "        return max(actions_rewards, key=lambda x: x[1])[0]\n",
    "\n",
    "    def _learn(self, prev_state, action, new_state):\n",
    "        \"\"\"update Q-value for the last taken action\"\"\"\n",
    "        if new_state not in self.Q:\n",
    "            self.Q[new_state] = {a: 0 for a in self.actions(new_state)}\n",
    "        reward = self.R(new_state)\n",
    "        self.reward += reward\n",
    "        self.Q[prev_state][action] = self.Q[prev_state][action] + self.learning_rate * (reward + self.discount * max(self.Q[new_state].values()) - self.Q[prev_state][action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c713f4e05ba5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# try discount=0.1 and discount=0.9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstarting_positions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQLearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "from pprint import pprint\n",
    "\n",
    "# try discount=0.1 and discount=0.9\n",
    "pos = random.choice(env.starting_positions)\n",
    "agent = QLearner(pos, env, env.reward, discount=0.9, learning_rate=1)\n",
    "\n",
    "print('without training...')\n",
    "agent.explore = 0\n",
    "for i in range(100):\n",
    "    game_over = False\n",
    "    # start at a random position\n",
    "    pos = random.choice(env.starting_positions)\n",
    "    agent.reset(pos)\n",
    "    if i > 0:\n",
    "        print agent.state, agent.reward         \n",
    "    while not game_over:\n",
    "        agent.step()\n",
    "        if i > 0:\n",
    "            print  i, agent.state, agent.reward        \n",
    "        game_over = env.is_terminal_state(agent.state)\n",
    "    print('reward:', agent.reward)\n",
    "\n",
    "print('training...')\n",
    "episodes = 500\n",
    "agent.explore = 0.5\n",
    "for i in range(episodes):\n",
    "    #print('episode:', i)\n",
    "    game_over = False\n",
    "    steps = 0\n",
    "\n",
    "    # start at a random position\n",
    "    pos = random.choice(env.starting_positions)\n",
    "    agent.reset(pos)\n",
    "    while not game_over:\n",
    "        agent.step()\n",
    "        steps += 1\n",
    "        game_over = env.is_terminal_state(agent.state)\n",
    "\n",
    "# print out the agent's Q table\n",
    "print('learned Q table:')\n",
    "for pos, vals in agent.Q.items():\n",
    "    print('{} -> {}'.format(pos, vals))\n",
    "\n",
    "# let's see how it does\n",
    "print('after training...')\n",
    "agent.explore = 0\n",
    "for i in range(20):\n",
    "    # start at a random position\n",
    "    pos = random.choice(env.starting_positions)\n",
    "    agent.reset(pos)\n",
    "    game_over = False\n",
    "    print agent.state, agent.reward\n",
    "    #pprint(agent.Q[agent.state])     \n",
    "    while not game_over:\n",
    "        agent.step()\n",
    "        print agent.state, agent.reward\n",
    "        #pprint(agent.Q[agent.state])    \n",
    "        game_over = env.is_terminal_state(agent.state)\n",
    "    print('reward:', agent.reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [dsi]",
   "language": "python",
   "name": "Python [dsi]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
